import json
import os
import subprocess
import sys
import csv

# --- CONFIGURATION ---
DATASET_ROOT = "dataset_root"
GRAPH_DIR = os.path.join(DATASET_ROOT, "graphs")
RAW_GRAPH_DIR = os.path.join(GRAPH_DIR, "raw_graphs")
CLUSTERED_GRAPH_DIR = os.path.join(GRAPH_DIR, "clustered_graphs")
LOG_FILE = os.path.join(DATASET_ROOT, "experiment_log.csv")

VENV_PYTHON = "./venv/bin/python3" 

# Number of runs
NUM_ITERATIONS = 1 

def setup_directories():
    """Ensures the folder structure exists before starting."""
    os.makedirs(DATASET_ROOT, exist_ok=True)
    os.makedirs(GRAPH_DIR, exist_ok=True)
    os.makedirs(CLUSTERED_GRAPH_DIR, exist_ok=True)
    os.makedirs(RAW_GRAPH_DIR, exist_ok=True)


    if not os.path.exists(LOG_FILE):
        with open(LOG_FILE, "w", newline="") as f:
            writer = csv.writer(f)
            header = [
                "run_id", "placement_id", "design_name",
                "aspect_ratio", "core_util", "density", "synth_strategy",
                "io_mode", "time_driven", "routability_driven",
                "cts_max_wire", "cts_buf_dist", "cts_cluster_size", "cts_cluster_dia",
                "skew_setup", "skew_hold", "power_total", "setup_slack", "wirelength",
                "raw_graph_path", "cluster_graph_path"
            ]
            writer.writerow(header)

    print(f"Directories Created in: {DATASET_ROOT} and Log file at {LOG_FILE}")

def get_next_run_id():
    """Parses the CSV to find the highest integer run_id and increments it."""
    if not os.path.exists(LOG_FILE):
        return 1
    
    max_id = 0
    try:
        with open(LOG_FILE, "r") as f:
            reader = csv.reader(f)
            header = next(reader, None) # Skip header
            for row in reader:
                # Check if row is not empty and first column is a valid integer
                if row and row[0].isdigit():
                    current_id = int(row[0])
                    if current_id > max_id:
                        max_id = current_id
    except Exception as e:
        print(f"Error reading CSV for ID: {e}")
        return 1
        
    return max_id + 1


def get_latest_run_tag():
    """Reads the tag from the file generated by Step 1."""
    try:
        with open("latest_run.txt", "r") as f:
            return f.readline().strip()
    except FileNotFoundError:
        return None
    
def log_data_to_csv(run_id, placement_id, raw_graph_path, cluster_graph_path):
    """Logs only the ID and file paths, leaving metrics/knobs blank for now."""

    # 1. Load the Placement Stats JSON generated by script 1
    pl_stats = {}
    try:
        with open("latest_stats.json", "r") as f:
            pl_stats = json.load(f)
    except FileNotFoundError:
        print("'latest_stats.json' not found. Logging with empty placement data.")

    base_cts_dir = os.path.join("runs", placement_id, "CTS-experiments")
    current_db_id = run_id
    with open(LOG_FILE, "a", newline="") as f:
        writer = csv.writer(f)
        
        for i in range(1, 11): # Loop 1 to 10
            cts_folder = os.path.join(base_cts_dir, f"CTS-{i}")
            knob_file = os.path.join(cts_folder, "knobs.json")
            
            # Default empty knobs if file missing (e.g. run crashed)
            cts_knobs = {}
            if os.path.exists(knob_file):
                with open(knob_file, "r") as kf:
                    cts_knobs = json.load(kf)
            else:
                print(f"Warning: Knobs for CTS-{i} not found. Skipping or logging empty.")

            # 3. Construct the Row
            row = [
                current_db_id,          # Incremental Unique ID
                placement_id,
                pl_stats.get("design_name", ""),
                pl_stats.get("aspect_ratio", ""),
                pl_stats.get("core_util", ""),
                pl_stats.get("density", ""),
                pl_stats.get("synth_strategy", ""),
                pl_stats.get("io_mode", ""),
                pl_stats.get("time_driven", ""),
                pl_stats.get("routability_driven", ""),
                
                # CTS Knobs (Mapped from JSON to CSV columns)
                cts_knobs.get("CTS_CLK_MAX_WIRE_LENGTH", ""),
                cts_knobs.get("CTS_DISTANCE_BETWEEN_BUFFERS", ""),
                cts_knobs.get("CTS_SINK_CLUSTERING_SIZE", ""),
                cts_knobs.get("CTS_SINK_CLUSTERING_MAX_DIAMETER", ""),
                
                "", # skew_setup (Will be filled by parsing reports later)
                "", # skew_hold
                "", # power_total
                "", # setup_slack
                "", # wirelength
                
                raw_graph_path,      # Same graph for all 10
                cluster_graph_path   # Same graph for all 10
            ]
            
            writer.writerow(row)
            current_db_id += 1 # Increment ID for next row

    print(f"âœ… Logged 10 entries for {placement_id} (IDs {run_id} to {current_db_id-1})")
    

def run_pipeline():
    setup_directories()
    
    # Check VENV
    if not os.path.exists(VENV_PYTHON):
        print(f"Error. Create a VENV at {VENV_PYTHON} with required packages.")
        return

    for i in range(NUM_ITERATIONS):
        print(f"iteration number: {i+1}")

        print(" 1. Running Placement Generator...")
        subprocess.run(["python3", "scripts/1-gen-placement.py"], check=True)
        placement_id = get_latest_run_tag()
        if not placement_id:
            print("Could not get placement ID.")
            break


        subprocess.run(["python3", "scripts/2-gen-saif.py", placement_id], check=True)

        raw_path = os.path.join(RAW_GRAPH_DIR, f"{placement_id}_raw.pt")
        subprocess.run([VENV_PYTHON, "scripts/three_def_dict_to_raw_graph.py", placement_id], check=True)

        cluster_path = os.path.join(CLUSTERED_GRAPH_DIR, f"{placement_id}_clustered.pt")
        subprocess.run([VENV_PYTHON, "scripts/4-graph_to_cluster.py", placement_id], check=True)

        #After running the graph generation scripts, verify files exist
        if not os.path.exists(raw_path) or not os.path.exists(cluster_path):
            RuntimeError("Graph files not found after generation.")
            break

        subprocess.run(["python3", "scripts/5-run-cts.py", placement_id], check=True)

        # subprocess.run(["python3", "scripts/6-parse-cts-reports.py", placement_id], check=True)
        next_id = get_next_run_id()
        log_data_to_csv(next_id, placement_id, raw_path, cluster_path)


        print("Run Complete.")

if __name__ == "__main__":
    run_pipeline()